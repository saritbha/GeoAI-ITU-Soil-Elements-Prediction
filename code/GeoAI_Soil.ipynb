{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjyzSWNw0lvw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import cross_validate, train_test_split, KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from scipy.signal import savgol_filter\n",
        "from scipy.optimize import minimize\n",
        "from scipy.stats import skew, kurtosis\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaUkiXzD0rax"
      },
      "outputs": [],
      "source": [
        "class BaselineRegressor:\n",
        "    \"\"\"\n",
        "    Baseline regressor, which calculates the mean value of the target from the training\n",
        "    data and returns it for each testing sample.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.mean = 0\n",
        "\n",
        "    def fit(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        self.mean = np.mean(y_train, axis=0)\n",
        "        self.classes_count = y_train.shape[1]\n",
        "        return self\n",
        "\n",
        "    def predict(self, X_test: np.ndarray):\n",
        "        return np.full((len(X_test), self.classes_count), self.mean)\n",
        "\n",
        "\n",
        "class SpectralCurveFiltering():\n",
        "    \"\"\"\n",
        "    Create a histogram (a spectral curve) of a 3D cube, using the merge_function\n",
        "    to aggregate all pixels within one band. The return array will have\n",
        "    the shape of [CHANNELS_COUNT]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, merge_function = np.mean):\n",
        "        self.merge_function = merge_function\n",
        "\n",
        "    def __call__(self, sample: np.ndarray):\n",
        "\n",
        "        sample = sample.reshape(sample.shape[0],-1)\n",
        "\n",
        "\n",
        "        band_mean = np.mean(sample, axis=1)\n",
        "        band_std = np.std(sample, axis=1)\n",
        "        band_var = np.var(sample, axis=1)\n",
        "        band_skew = skew(sample, axis=1)\n",
        "        band_kurt = kurtosis(sample, axis=1)\n",
        "        band_area = np.array([sample.shape[1]]*sample.shape[0])\n",
        "        return np.array([band_mean, band_std, band_var, band_skew, band_kurt, band_area ]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsx1BlO51B__",
        "outputId": "42ddae3a-f002-4e12-d0f8-69d4c77db089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape: (1732, 150, 6)\n",
            "Test data shape: (1154, 150, 6)\n"
          ]
        }
      ],
      "source": [
        "def load_data(directory: str):\n",
        "    \"\"\"Load each cube, reduce its dimensionality and append to array.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Directory to either train or test set\n",
        "    Returns:\n",
        "        [type]: A list with spectral curve for each sample.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    filtering = SpectralCurveFiltering()\n",
        "    all_files = np.array(\n",
        "        sorted(\n",
        "            glob(os.path.join(directory, \"*.npz\")),\n",
        "            key=lambda x: int(os.path.basename(x).replace(\".npz\", \"\")),\n",
        "        )\n",
        "    )\n",
        "    for file_name in all_files:\n",
        "        with np.load(file_name) as npz:\n",
        "\n",
        "            arr = np.ma.MaskedArray(**npz)\n",
        "        arr = filtering(arr)\n",
        "        data.append(arr)\n",
        "    return np.array(data)\n",
        "\n",
        "\n",
        "\n",
        "def load_gt(file_path: str):\n",
        "    \"\"\"Load labels for train set from the ground truth file.\n",
        "    Args:\n",
        "        file_path (str): Path to the ground truth .csv file.\n",
        "    Returns:\n",
        "        [type]: 2D numpy array with soil properties levels\n",
        "    \"\"\"\n",
        "    gt_file = pd.read_csv(file_path)\n",
        "    labels = gt_file[[\"P\", \"K\", \"Mg\", \"pH\"]].values\n",
        "    return labels\n",
        "\n",
        "\n",
        "X_train = load_data(\"train_data/train_data\")\n",
        "y_train = load_gt(\"train_data/train_gt.csv\")\n",
        "X_test = load_data(\"test_data\")\n",
        "\n",
        "print(f\"Train data shape: {X_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0rUAvjgX1GE3"
      },
      "outputs": [],
      "source": [
        "np.save('stats_train_bands', X_train)\n",
        "np.save('stats_test_bands', X_test)\n",
        "np.save('stats_train_label', y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9_wu29wr1piT"
      },
      "outputs": [],
      "source": [
        "# Define the Savitzky-Golay second-derivative function\n",
        "def sg_sd(x, window_length=7, polyorder=2):\n",
        "    # Apply the Savitzky-Golay filter to smooth the spectra\n",
        "    smoothed = savgol_filter(x, window_length, polyorder, axis=1)\n",
        "    # Calculate the second-derivative of the smoothed spectra\n",
        "    sd = np.gradient(np.gradient(smoothed, axis=1), axis=1)\n",
        "    return sd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nBYC1rDA1sQp"
      },
      "outputs": [],
      "source": [
        "# Load the formatted data\n",
        "train_bands = np.load('stats_train_bands.npy').transpose((0,2,1))  # transform into shape ([Lambda1 mean,...]*150 , [Lambda1 std,...]*150,... [....])\n",
        "test_bands = np.load('stats_test_bands.npy').transpose((0,2,1))\n",
        "train_elements = np.load('stats_train_label.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQmOgfMK1um3",
        "outputId": "ccf7a898-6445-4ebf-ac2f-f7befc8dfd92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1732, 751), (1154, 751))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_area = train_bands[:,-1,:].mean(axis=1).reshape(-1,1) # the last last column is the area dim\n",
        "test_area = test_bands[:,-1,:].mean(axis=1).reshape(-1,1)\n",
        "\n",
        "train_bands = train_bands[:,:-1,:].reshape(train_bands.shape[0],-1) # reshape into 2D matrix without the area dim\n",
        "test_bands = test_bands[:,:-1,:].reshape(test_bands.shape[0],-1)\n",
        "\n",
        "train_bands = np.hstack((train_bands, train_area)) # add the area column as the last column into the formatted 2D array\n",
        "test_bands = np.hstack((test_bands, test_area))\n",
        "\n",
        "# using red band 630 to 690; blue band 450 - 520; green band 520 - 600 and NIR band 760 - 900 (nm)\n",
        "min_wav = 462\n",
        "red_start, red_end = int((630 - min_wav)/3.2), int((690 - min_wav)/3.2)\n",
        "blue_end = int((520 - min_wav)/3.2)\n",
        "green_start, green_end = int((520 - min_wav)/3.2), int((600 - min_wav)/3.2)\n",
        "nir_start, nir_end = int((760 - min_wav)/3.2), int((900 - min_wav)/3.2)\n",
        "\n",
        "\n",
        "train_bands.shape, test_bands.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vjYi70PG1w-6"
      },
      "outputs": [],
      "source": [
        "# creating color and NIR bands using the MEAN only.\n",
        "\n",
        "train_blue = train_bands[:,0:blue_end].mean(1)\n",
        "train_green = train_bands[:,green_start:green_end].mean(1)\n",
        "train_red = train_bands[:,red_start:red_end].mean(1)\n",
        "train_nir = train_bands[:,nir_start:nir_end].mean(1)\n",
        "\n",
        "\n",
        "test_blue = test_bands[:,0:blue_end].mean(1)\n",
        "test_green = test_bands[:,green_start:green_end].mean(1)\n",
        "test_red = test_bands[:,red_start:red_end].mean(1)\n",
        "test_nir = test_bands[:,nir_start:nir_end].mean(1)\n",
        "\n",
        "train_nvdi = (train_nir - train_red)/(train_nir + train_red) # vegetation index\n",
        "train_pri = (train_green - train_blue)/(train_green + train_blue)\n",
        "train_evi = 2.5*(train_nir - train_red)/(train_nir + 6*train_red - 7.5*train_blue +1) # enhanced vegetation index\n",
        "train_savi = (train_nir - train_red)/(train_nir + train_red + 0.5)*(1 + 0.5) # soil index\n",
        "train_ratio1 = train_red/train_blue\n",
        "train_ratio2 = train_red/train_green\n",
        "train_ratio3 = train_blue/train_green\n",
        "train_ratio4 = train_blue/train_green\n",
        "train_ratio5 = train_green/train_red\n",
        "train_ratio6 = train_green/train_blue\n",
        "\n",
        "test_nvdi = (test_nir - test_red)/(test_nir + test_red)\n",
        "test_pri = (test_green - test_blue)/(test_green + test_blue)\n",
        "test_evi = 2.5*(test_nir - test_red)/(test_nir + 6*test_red - 7.5*test_blue +1)\n",
        "test_savi = (test_nir - test_red)/(test_nir + test_red + 0.5)*(1 +0.5)\n",
        "test_ratio1 = test_red/test_blue\n",
        "test_ratio2 = test_red/test_green\n",
        "test_ratio3 = test_blue/test_green\n",
        "test_ratio4 = test_blue/test_green\n",
        "test_ratio5 = test_green/test_red\n",
        "test_ratio6 = test_green/test_blue\n",
        "\n",
        "\n",
        "\n",
        "color_train = np.array([train_blue, train_green, train_red, train_nir, train_nvdi, train_evi, train_savi, train_ratio1,train_ratio2,train_ratio3,train_ratio4,train_ratio5,train_ratio6 ]).T\n",
        "color_test = np.array([test_blue, test_green, test_red, test_nir, test_nvdi, test_evi, test_savi, test_ratio1,test_ratio2,test_ratio3,test_ratio4,test_ratio5,test_ratio6]).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gkcZTpmH141J"
      },
      "outputs": [],
      "source": [
        "# join all the created features\n",
        "train_band_features = np.hstack((train_bands, color_train))\n",
        "test_band_features = np.hstack((test_bands, color_test))\n",
        "\n",
        "#  only the MEAN which is from (0 to 149th column) we'll be used and savgol filter with 2nd derivate applied to smoothen the signal\n",
        "train_band_features[:,0:150] = sg_sd(train_band_features[:,0:150], window_length=3, polyorder=1) # window length and polyorder were tuned\n",
        "test_band_features[:,0:150] = sg_sd(test_band_features[:,0:150], window_length=3, polyorder=1)\n",
        "\n",
        "# scale the features\n",
        "train_band_features = (train_band_features - train_band_features.mean(0))/train_band_features.std(0)\n",
        "test_band_features = (test_band_features - test_band_features.mean(0))/test_band_features.std(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BTTR4Hun16yu"
      },
      "outputs": [],
      "source": [
        "# Defining the parameters of the models, all were tuned using hyperopt optimization.\n",
        "\n",
        "forest_params = {'n_estimators': 203,'max_depth': 78,'max_leaf_nodes': 348,'min_samples_split': 6,'min_samples_leaf': 4,\n",
        "                 'min_weight_fraction_leaf': 0.0010047252525281973,'min_impurity_decrease': 0.0020686506542306835,'max_features': 0.9995836445306979,\n",
        "                 'random_state':42, 'n_jobs':-1\n",
        "                }\n",
        "\n",
        "etr_params = {'n_estimators': 102,'min_samples_split': 8,'min_samples_leaf': 2,'min_weight_fraction_leaf': 0.001467859820666249,\n",
        "              'min_impurity_decrease': 0.10117651856072411,'max_features': 0.96464832151798,'random_state':42, 'n_jobs':-1\n",
        "             }\n",
        "\n",
        "knn_params = {'n_neighbors': 54, 'leaf_size': 134, 'p': 5, 'weights':'distance', 'n_jobs':-1}\n",
        "\n",
        "lgb_params = {'boosting': 'gbdt','num_iterations': 173,'learning_rate': 0.01807409716203315,\n",
        "              'tree_learner': 'serial','num_leaves': 539,'max_depth': 6,'n_estimators': 236,'subsample_for_bin': 16269,'min_split_gain': 0.13801317023253418,\n",
        "              'min_child_weight': 0.6547238467990038,'min_child_samples': 5,'reg_alpha': 0.00015187650791215833,'reg_lambda': 0.0771509874666758,\n",
        "              'bagging_freq': 1,'bagging_fraction': 0.9202173572091822,'feature_fraction': 0.9721966537259545,'random_state':42, 'n_jobs':-1,'verbose':-1,\n",
        "             }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "raBqDtznAvBw"
      },
      "outputs": [],
      "source": [
        "# To make the predictions we'll use 4 fold cv and make predictions. oof predictions will also be used to finding weights to add the model outputs\n",
        "\n",
        "def oof_model(features, target, test_features, models, cv=4): # models is list of  tuples [(model_class, model_name, model_params),..., (...)]\n",
        "    test_predictions = []  # to store the predictions from different model\n",
        "    model_oofs = []\n",
        "    model_ytrues = []  # just stores the copies of y_true\n",
        "    print(\"Info from oof models\")\n",
        "    for model, name, param in models:\n",
        "        oof_predictions = np.zeros(target.shape)\n",
        "        y_true = np.zeros(target.shape)  # stores the target values\n",
        "        kf = KFold(n_splits=cv, shuffle=True, random_state=2023)\n",
        "        test_pred = []\n",
        "        for train_idx, valid_idx in kf.split(features):\n",
        "            train_x, valid_x = features[train_idx], features[valid_idx]\n",
        "            train_y, valid_y = target[train_idx], target[valid_idx]\n",
        "            if 'lgb' in name:\n",
        "                new_model = MultiOutputRegressor(model(**param))\n",
        "            else:\n",
        "                new_model = model(**param)\n",
        "            new_model.fit(train_x, train_y)\n",
        "            pred_y = new_model.predict(valid_x)\n",
        "\n",
        "            y_true[valid_idx] = valid_y\n",
        "            oof_predictions[valid_idx] = pred_y\n",
        "\n",
        "            test_pred.append(new_model.predict(test_features))\n",
        "\n",
        "        model_oofs.append(oof_predictions)\n",
        "        model_ytrues.append(y_true)\n",
        "\n",
        "        # print the errors and r2\n",
        "        rmse = np.sqrt(mean_squared_error(oof_predictions, y_true))\n",
        "        r2 = r2_score(oof_predictions, y_true)\n",
        "        # print(f\"{name} rmse : {rmse} -- {name} r2 : {r2}\")\n",
        "        test_predictions.append(np.mean(test_pred,axis=0))\n",
        "    print()\n",
        "    return test_predictions, model_oofs, model_ytrues\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0efPK64ZAwRs"
      },
      "outputs": [],
      "source": [
        "# define the models for oof prediction\n",
        "models = [(RandomForestRegressor, 'forest', forest_params),\n",
        "          (ExtraTreesRegressor,  'etr', etr_params),\n",
        "          (KNeighborsRegressor, 'knn', knn_params),\n",
        "          (LGBMRegressor, 'lgb', lgb_params)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7U5En5tAzWP",
        "outputId": "0894820f-75b3-4f46-f08f-a8d451795c66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Info from oof models\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# for observation, we'll split the data into train and validation\n",
        "train_bands, valid_bands, train_target, valid_target = train_test_split(train_band_features, train_elements, test_size=0.3, random_state=42)\n",
        "\n",
        "# get the model predictions from oof_models\n",
        "valid_predictions_oof, train_oofs, train_Ys = oof_model(train_bands, train_target, valid_bands, models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXDJjt0NA1A9",
        "outputId": "8592324f-307c-4ca0-855a-77731a9967a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "forest - P2O5 rmse : 24.30678466350733\n",
            "forest - Potassium rmse : 55.1070960440621\n",
            "forest - Magnesium rmse : 37.88845891312706\n",
            "forest - pH rmse : 0.24204854850833096\n",
            "etr - P2O5 rmse : 24.446533197901775\n",
            "etr - Potassium rmse : 54.68751019076692\n",
            "etr - Magnesium rmse : 38.04175478095468\n",
            "etr - pH rmse : 0.23960802389719812\n",
            "knn - P2O5 rmse : 24.692258313266873\n",
            "knn - Potassium rmse : 57.72475193935712\n",
            "knn - Magnesium rmse : 38.98221668720845\n",
            "knn - pH rmse : 0.24941461331799802\n",
            "lgb - P2O5 rmse : 24.345389932211912\n",
            "lgb - Potassium rmse : 54.8646523234527\n",
            "lgb - Magnesium rmse : 37.74227988897312\n",
            "lgb - pH rmse : 0.2361300877404062\n",
            "Mean model - P2O5  rmse : 25.066388715950136\n",
            "Mean model - Potassium  rmse : 59.11923439038279\n",
            "Mean model - Magnesium  rmse : 39.251741731664985\n",
            "Mean model - pH  rmse : 0.25723848447413805\n"
          ]
        }
      ],
      "source": [
        "# lets check the errors of each model for each of the outputs\n",
        "\n",
        "model_order = ['forest','etr','knn','lgb']\n",
        "for i, model_name in enumerate(model_order):\n",
        "    oof_pred =valid_predictions_oof[i]\n",
        "    for j,tar in enumerate(['P2O5','Potassium','Magnesium','pH']):\n",
        "        oof_rmse = np.sqrt(mean_squared_error(oof_pred[:,j],valid_target[:,j]))\n",
        "        print(f\"{model_name} - {tar} rmse : {oof_rmse}\")\n",
        "\n",
        "# print mean model prediction and the errors\n",
        "mean_pred = np.array([train_elements.mean(0)]*len(valid_target))\n",
        "for i,tar in enumerate(['P2O5','Potassium','Magnesium','pH']):\n",
        "    mean_rmse = np.sqrt(mean_squared_error(mean_pred[:,i],valid_target[:,i]))\n",
        "    print(f\"Mean model - {tar}  rmse : {mean_rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EFMJ69_1A3e7"
      },
      "outputs": [],
      "source": [
        "# from above we can see that some models perform particulary well on specific target,\n",
        "# Ex:- Lightgbm has lowest mse among all the models for pH;\n",
        "# ExtraTreesRegressor has lowest mse among all the models for Potassium;\n",
        "# RandomForest has lowest for Magnesium, etc.\n",
        "\n",
        "# Therefore i decided to use weights not only for model prediction as whole but also each target.\n",
        "# In total there will be (Num. of model X Num. of target) weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OgLVSczZA5or"
      },
      "outputs": [],
      "source": [
        "# Combine the predictions into a single function\n",
        "def combined_predictions(weights, predictions):\n",
        "    predictions = np.array(predictions).transpose(1,2,0) # to shape (num of samples, num of target, num of model)\n",
        "    custom_weights = np.array(weights).reshape(predictions.shape[1], predictions.shape[2]) # to shape (num of target, num of model)\n",
        "    pred  = []\n",
        "    for i, w_row in enumerate(custom_weights[:]):\n",
        "        pred.append(np.sum(w_row*predictions[:,i,:], axis=1))\n",
        "    return np.array(pred).T\n",
        "\n",
        "# Define the objective function to minimize (mean squared error + r2)\n",
        "def objective(weights, predictions, actual):\n",
        "    pred = combined_predictions(weights, predictions)\n",
        "    mse = mean_squared_error(actual, pred)\n",
        "    return mse\n",
        "\n",
        "def weighted_prediction(model_oofs, model_true, num_target=4):\n",
        "    # Initialize weights\n",
        "    initial_weights = np.ones(len(model_oofs)*num_target)\n",
        "\n",
        "    # Optimize the weights\n",
        "    result = minimize(objective, initial_weights, args=(model_oofs, model_true), method='L-BFGS-B')\n",
        "\n",
        "    # Get the optimized weights\n",
        "    optimized_weights = result.x\n",
        "\n",
        "    return optimized_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WzgUn4gZA7OZ"
      },
      "outputs": [],
      "source": [
        "# We'll be using the train_oofs and train_targets to find the optimal weights for\n",
        "model_true  = np.mean(train_Ys, 0) # taking mean changes nothing, there are just copies the train_target values\n",
        "\n",
        "# get the optimized weights\n",
        "optimized_weights = weighted_prediction(train_oofs, model_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D2hf9NnA8-m",
        "outputId": "d053934f-6c29-4280-c5a0-5a8988d94f5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.71135591,  0.11715665, -0.17557001,  0.33810638,  0.21707926,\n",
              "        0.33871198, -0.04713769,  0.48723512, -0.06140541,  0.11978116,\n",
              "        0.17859078,  0.76470645,  0.24959966,  0.2497377 ,  0.25102225,\n",
              "        0.25042097])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# every consecutive four weights represents the weights to different model in the order (forest, etr, knn, and lgb)\n",
        "# as you can see the the last weight correspond to the lgb weight for pH and it has the highest among all and as we saw earlier lgb gave lowest MSE,\n",
        "# similarly the first weight corresponds to the forest model prediction of P2O5 and as we saw earlier this again gets the highest weight, so on and so forth.\n",
        "optimized_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV6Qor1LA-ju",
        "outputId": "76ce5add-0c95-45e7-e5c2-684b8e2498d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE with oof predictions  35.557506676076464\n",
            "RMSE with mean model predictions  37.63035051248978\n"
          ]
        }
      ],
      "source": [
        "# Lets compare the predictions\n",
        "\n",
        "oof_valid_pred = np.mean(valid_predictions_oof,0)\n",
        "mean_valid_pred = np.array([train_elements.mean(0)]*len(valid_target))\n",
        "\n",
        "print(\"RMSE with oof predictions \",np.sqrt(mean_squared_error(oof_valid_pred, valid_target)))\n",
        "print(\"RMSE with mean model predictions \",np.sqrt(mean_squared_error(mean_valid_pred, valid_target)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pihTzqVjBAS3",
        "outputId": "f0afdf63-c200-453f-8539-cb5a18d62a32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE with weighted oof predictions  35.292709539050925\n",
            "RMSE with mean model predictions  37.63035051248978\n"
          ]
        }
      ],
      "source": [
        "# lets make predictions using the weights and see the results using the valid_prediction_oof and valid_prediction_cv\n",
        "weighted_oof_pred = combined_predictions(optimized_weights, valid_predictions_oof)\n",
        "\n",
        "print(\"RMSE with weighted oof predictions \",np.sqrt(mean_squared_error(weighted_oof_pred, valid_target)))\n",
        "print(\"RMSE with mean model predictions \",np.sqrt(mean_squared_error(mean_valid_pred, valid_target)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtriSPOCBCAG",
        "outputId": "40deab0f-0a27-47b0-9121-abf0a4951c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Info from oof models\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.9202173572091822, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9202173572091822\n",
            "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
            "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
            "[LightGBM] [Warning] feature_fraction is set=0.9721966537259545, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9721966537259545\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now that we have our weights lets make prediction for the test data, for this we'll use the whole training data and make the prediction\n",
        "\n",
        "test_prediciton, _, _ = oof_model(train_band_features, train_elements, test_band_features, models)\n",
        "\n",
        "weighted_test_prediction = combined_predictions(optimized_weights, test_prediciton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FKiK_BXCBEhr"
      },
      "outputs": [],
      "source": [
        "# Before submitting we need to normalize the values\n",
        "mean_norm = train_elements.mean(0)\n",
        "weighted_test_prediction /= mean_norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "344ng1HbBHku",
        "outputId": "cadf78c2-6125-4c74-e682-67fe368b03ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  sample_index    Target\n",
            "0          0_P  0.961311\n",
            "1          0_K  0.961130\n",
            "2         0_Mg  0.974144\n",
            "3         0_pH  1.012643\n",
            "4          1_P  0.989327\n"
          ]
        }
      ],
      "source": [
        "# load the sample submission\n",
        "sample_sub = pd.read_csv('SampleSubmission.csv.csv')\n",
        "model_sub = pd.DataFrame(data = weighted_test_prediction, columns=[\"P\", \"K\", \"Mg\", \"pH\"]).reset_index()\n",
        "\n",
        "\n",
        "model_sub.columns = ['sample_index', 'P', 'K', 'Mg', 'pH']\n",
        "model_sub = model_sub.melt(id_vars=['sample_index'], value_vars=['P', 'K', 'Mg', 'pH'])\n",
        "model_sub.sort_values('sample_index',inplace=True)\n",
        "\n",
        "model_sub['sample_index'] = model_sub['sample_index'].astype('str')+\"_\"+model_sub['variable']\n",
        "model_sub.drop('variable', inplace=True, axis=1)\n",
        "model_sub.columns = ['sample_index', 'Target']\n",
        "model_sub.reset_index(inplace=True, drop=True)\n",
        "\n",
        "sample_sub.drop('Target',axis=1,inplace=True)\n",
        "sample_sub = sample_sub.merge(model_sub, on='sample_index', how='left')\n",
        "print(sample_sub.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Pj6x1l1GBJWr"
      },
      "outputs": [],
      "source": [
        "# save the file\n",
        "sample_sub.to_csv('Submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
